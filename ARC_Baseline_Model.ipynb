{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e262b5f0-cf8b-42c5-bcdd-4426eb1f45ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../data/training\n",
      "Loaded 400 tasks from training\n",
      "Loading data from: ../data/evaluation\n",
      "Loaded 400 tasks from evaluation\n",
      "Prepared 1302 input-output pairs\n",
      "Prepared 1363 input-output pairs\n",
      "Number of training samples: 1302\n",
      "Number of testing samples: 1363\n",
      "Training dataset size: 1302\n",
      "Testing dataset size: 1363\n",
      "Number of training batches: 41\n",
      "Number of testing batches: 43\n",
      "Epoch 1/100, Loss: 24.37352289804598\n",
      "Epoch 2/100, Loss: 17.74205887026903\n",
      "Epoch 3/100, Loss: 17.536770332150343\n",
      "Epoch 4/100, Loss: 17.999961876287692\n",
      "Epoch 5/100, Loss: 18.00680444298721\n",
      "Epoch 6/100, Loss: 17.656003998547064\n",
      "Epoch 7/100, Loss: 17.511576419923365\n",
      "Epoch 8/100, Loss: 17.802117673362172\n",
      "Epoch 9/100, Loss: 17.37524327999208\n",
      "Epoch 10/100, Loss: 17.622005532427533\n",
      "Epoch 11/100, Loss: 17.8683358401787\n",
      "Epoch 12/100, Loss: 17.314264413787097\n",
      "Epoch 13/100, Loss: 17.786755910733852\n",
      "Epoch 14/100, Loss: 17.42245895106618\n",
      "Epoch 15/100, Loss: 17.06242840464522\n",
      "Epoch 16/100, Loss: 16.85732957793445\n",
      "Epoch 17/100, Loss: 17.232270659469975\n",
      "Epoch 18/100, Loss: 17.164629052324994\n",
      "Epoch 19/100, Loss: 17.05923734060148\n",
      "Epoch 20/100, Loss: 16.906681525998\n",
      "Epoch 21/100, Loss: 17.02277923211819\n",
      "Epoch 22/100, Loss: 16.84313481028487\n",
      "Epoch 23/100, Loss: 16.873320091061476\n",
      "Epoch 24/100, Loss: 16.78418085051746\n",
      "Epoch 25/100, Loss: 16.994142951034917\n",
      "Epoch 26/100, Loss: 17.46745656176311\n",
      "Epoch 27/100, Loss: 16.815203806249105\n",
      "Epoch 28/100, Loss: 17.050150243247426\n",
      "Epoch 29/100, Loss: 17.291653144650343\n",
      "Epoch 30/100, Loss: 16.918271111278997\n",
      "Epoch 31/100, Loss: 16.68994647700612\n",
      "Epoch 32/100, Loss: 16.626631643713974\n",
      "Epoch 33/100, Loss: 16.75342024826422\n",
      "Epoch 34/100, Loss: 16.862450134463426\n",
      "Epoch 35/100, Loss: 16.750250281357182\n",
      "Epoch 36/100, Loss: 16.76718800242354\n",
      "Epoch 37/100, Loss: 16.743829261965868\n",
      "Epoch 38/100, Loss: 16.692476295843356\n",
      "Epoch 39/100, Loss: 16.68968784518358\n",
      "Epoch 40/100, Loss: 16.754732713466737\n",
      "Epoch 41/100, Loss: 16.71898644144942\n",
      "Epoch 42/100, Loss: 16.595681958082245\n",
      "Epoch 43/100, Loss: 16.4740003957981\n",
      "Epoch 44/100, Loss: 16.676953734421147\n",
      "Epoch 45/100, Loss: 16.516508846748167\n",
      "Epoch 46/100, Loss: 16.59188686929098\n",
      "Epoch 47/100, Loss: 16.645842877829946\n",
      "Epoch 48/100, Loss: 16.455774074647486\n",
      "Epoch 49/100, Loss: 16.593709247868237\n",
      "Epoch 50/100, Loss: 16.53071173225961\n",
      "Epoch 51/100, Loss: 16.520541167840726\n",
      "Epoch 52/100, Loss: 16.642463288656096\n",
      "Epoch 53/100, Loss: 16.641471304544588\n",
      "Epoch 54/100, Loss: 16.51476799569479\n",
      "Epoch 55/100, Loss: 16.4351607532036\n",
      "Epoch 56/100, Loss: 16.440860678509967\n",
      "Epoch 57/100, Loss: 16.406755400867\n",
      "Epoch 58/100, Loss: 16.413491807332854\n",
      "Epoch 59/100, Loss: 16.539281775311725\n",
      "Epoch 60/100, Loss: 16.55553715403487\n",
      "Epoch 61/100, Loss: 16.66337745945628\n",
      "Epoch 62/100, Loss: 16.696010263954722\n",
      "Epoch 63/100, Loss: 16.55986590501739\n",
      "Epoch 64/100, Loss: 16.38725327282417\n",
      "Epoch 65/100, Loss: 16.510107854517496\n",
      "Epoch 66/100, Loss: 16.361020925568372\n",
      "Epoch 67/100, Loss: 16.50053247591344\n",
      "Epoch 68/100, Loss: 16.373712655974597\n",
      "Epoch 69/100, Loss: 16.539969095369663\n",
      "Epoch 70/100, Loss: 16.431998741335985\n",
      "Epoch 71/100, Loss: 16.383072038976156\n",
      "Epoch 72/100, Loss: 16.280837500967632\n",
      "Epoch 73/100, Loss: 16.260838601647354\n",
      "Epoch 74/100, Loss: 16.429768283192704\n",
      "Epoch 75/100, Loss: 16.459846054635396\n",
      "Epoch 76/100, Loss: 16.436979549687084\n",
      "Epoch 77/100, Loss: 16.27359080896145\n",
      "Epoch 78/100, Loss: 16.443108628435834\n",
      "Epoch 79/100, Loss: 16.419117764728824\n",
      "Epoch 80/100, Loss: 16.365214952608433\n",
      "Epoch 81/100, Loss: 16.376641831746916\n",
      "Epoch 82/100, Loss: 16.39002541797917\n",
      "Epoch 83/100, Loss: 16.408532793928938\n",
      "Early stopping at epoch 83\n",
      "Training complete!\n",
      "Test Loss: 16.353638427202092\n",
      "Model saved. Now go conquer the world with your ARC-solving prowess! ðŸš€ðŸ˜ˆ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.transforms import RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n",
    "\n",
    "def load_arc_data(directory):\n",
    "    tasks = []\n",
    "    full_path = os.path.join('..', 'data', directory)\n",
    "    print(f\"Loading data from: {full_path}\")\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"Directory does not exist: {full_path}\")\n",
    "        return tasks\n",
    "    for filename in os.listdir(full_path):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(full_path, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                tasks.append(json.load(f))\n",
    "    print(f\"Loaded {len(tasks)} tasks from {directory}\")\n",
    "    return tasks\n",
    "\n",
    "def prepare_data(tasks):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        for train in task['train']:\n",
    "            inputs.append(np.array(train['input']))\n",
    "            outputs.append(np.array(train['output']))\n",
    "    print(f\"Prepared {len(inputs)} input-output pairs\")\n",
    "    return inputs, outputs\n",
    "\n",
    "# Load and prepare data\n",
    "train_tasks = load_arc_data('training')\n",
    "test_tasks = load_arc_data('evaluation')\n",
    "\n",
    "train_inputs, train_outputs = prepare_data(train_tasks)\n",
    "test_inputs, test_outputs = prepare_data(test_tasks)\n",
    "\n",
    "# Convert to tensors\n",
    "train_inputs = [torch.tensor(arr, dtype=torch.float32) for arr in train_inputs]\n",
    "train_outputs = [torch.tensor(arr, dtype=torch.float32) for arr in train_outputs]\n",
    "test_inputs = [torch.tensor(arr, dtype=torch.float32) for arr in test_inputs]\n",
    "test_outputs = [torch.tensor(arr, dtype=torch.float32) for arr in test_outputs]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_inputs)}\")\n",
    "print(f\"Number of testing samples: {len(test_inputs)}\")\n",
    "\n",
    "class ARCDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs, transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = self.inputs[idx]\n",
    "        output_image = self.outputs[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            output_image = self.transform(output_image)\n",
    "        \n",
    "        return input_image, output_image\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.unsqueeze(0) if x.ndim == 2 else x),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),\n",
    "    transforms.Resize((32, 32)),\n",
    "    RandomRotation(degrees=90),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ARCDataset(train_inputs, train_outputs, transform=transform)\n",
    "test_dataset = ARCDataset(test_inputs, test_outputs, transform=transform)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print number of batches\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "\n",
    "# Define the model\n",
    "class ARCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ARCModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 3, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.upsample(torch.relu(self.conv4(x)))\n",
    "        x = self.upsample(torch.relu(self.conv5(x)))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ARCModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "    \n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_arc_model.pth\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "print(\"Model saved. Now go conquer the world with your ARC-solving prowess! ðŸš€ðŸ˜ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079d1be-342d-48d3-8e67-a73f40ba2ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
